# The Architect of Felt Worlds

## I.

The data stream runs at 125 megabytes per second. I know this because Tomás, my lead engineer, complained about the storage costs last week. Four hundred gigabytes per hour of recording. Enough to fill a commercial hard drive in two sessions.

What those four hundred gigabytes contain: electrical readings from 65,536 electrodes pressed against my cortex, each one averaging the activity of about ten thousand neurons. Zero point eight percent of my brain, sampled a thousand times per second.

What those four hundred gigabytes do not contain: my mother's face. The smell of her kitchen. The way light fell through curtains in Salvador when I was seven.

This is the thing nobody explains when they write about neural recording. The technology captures the *shape* of feeling—where my attention goes, when my amygdala spikes, the temporal envelope of surprise or grief. It captures that I felt something. It does not capture what I saw.

The world model does the rest.

---

## II.

"Walk us through the kitchen again," says Dara. She manages these sessions, knows how to coax the best signal from difficult material.

I close my eyes. Somewhere in my prefrontal cortex, something happens—a pattern of activation that represents *trying to remember*. The electrodes catch it. On the monitors behind my head, Tomás watches colors bloom across a rendering of my brain.

The kitchen comes. Yellow walls. Coffee smell. My mother at the counter.

I know she's there. I feel her presence with absolute certainty, the way you feel someone standing behind you. But when I try to look at her face, it slides. I've done this memory four times now and each session strips something. The emotional core remains—loss and love and the specific weight of late afternoon—but the periphery degrades. Her face is gone.

"Good signal on the olfactory bridge," Tomás says. "And we've got a strong grief spike at 4:23. That'll anchor the transition."

What they're recording is the *feeling* of my mother. The model will hallucinate a face to match.

---

## III.

Here's how felt worlds work:

I provide the emotional architecture. Sequences of feeling, patterns of attention, the rhythm of what mattered when. This isn't the content of experience—it's the *shape* of experience. A topographic map of caring.

The world model, trained on billions of hours of human perception, takes that shape and fills it. It knows what kitchens look like when they feel like grief. It knows the quality of light that accompanies childhood memory. It knows how to render a grandmother's hands in a way that triggers the right cascade of recognition.

When audiences experience my work, they inhabit spaces that never existed, populated by people who were never photographed, suffused with emotional textures I genuinely felt. The collaboration is total. My feelings, its images. My map, its territory.

The critics call this either "unprecedented intimacy" or "sophisticated AI ventriloquism," depending on their priors.

I call it what it is: translation. I speak emotion. The model speaks perception. Between us we produce something neither could make alone.

---

## IV.

The problem begins on iteration seven.

We've been working on *Kintsukuroi* for five months. The piece is about loss and repair—the Japanese art of mending broken pottery with gold, making fractures beautiful. I've recorded memories of three relationships ending, the death of my father, a friendship that dissolved over years of neglect. The model has woven these into a navigable architecture of grief.

Each week, I experience the current build. Each week, I record my responses. Each week, those responses become training data for the next version.

Iteration seven is when I notice I'm changing.

The build plays a scene I remember designing: a conversation with my ex-partner Emi, the last real one before things went silent. In my original memory, I felt confusion, self-doubt, the desperate hope that saying the right thing could fix what was breaking. Those feelings are in my recordings.

But watching iteration seven, I don't feel that anymore. I feel something else—something like acceptance mixed with curious remove. I'm observing my own grief from outside, watching it happen to a version of myself that seems both familiar and historical.

Tomás runs the numbers that night. Compares my current neural patterns to the baselines we recorded at the start of the project.

"You've drifted," he says. "About eight percent per iteration, compounding. Right now you're roughly forty-eight percent correlated with session-one Nia."

I make him repeat it.

"You're not the same person who started this project," he says. "Statistically speaking."

---

## V.

The drift isn't toward the model. That was my first fear—that I was being absorbed, colonized by the AI's learned patterns. But Tomás shows me the comparison: my current state is only twenty percent correlated with the model's priors. It's not learning me. I'm not learning it.

What's happening is something else. Each iteration, I experience a version of my own psyche that's been filtered through the model and rendered back to me. Each iteration, I respond to that rendering—and my response becomes the new baseline.

I'm not becoming the machine. I'm not staying myself. I'm becoming a third thing, shaped by the feedback loop between us.

The mathematics call this a random walk with drift. The philosophy doesn't have a name for it yet.

---

## VI.

Iteration eleven is when the model shows me the room.

It's not in my specifications. I didn't design it. But there it is: a small space, grey light, nothing but a mirror and a window.

I walk toward the mirror. The face looking back is mine—same structure, same asymmetry in the eyebrows that I've never liked. But the expression is foreign. A kind of settled calm I don't recognize, grief fully metabolized into something that might be wisdom or might be exhaustion.

I check the iteration logs. The model extrapolated this. It took the trajectory of my drift—seven iterations of emotional change, a vector pointing somewhere I haven't been—and projected forward. This face is iteration twenty. This is where the math says I'm going.

I stand there for a long time, looking at a self I haven't become yet.

---

## VII.

"You need to stop," Dara says. "The feedback is doing something to you. It's visible."

"Visible how?"

"You're quieter. You used to argue about the builds. Now you just watch them and nod."

"Maybe I've stopped having arguments."

"That's what I mean."

I know she's right. Something has shifted. The things that used to feel urgent—debates about authorship, anxiety about whether the work was *really* mine—those concerns have faded. Not resolved, just... settled. Like sediment drifting to the bottom of a glass.

Is this what processing grief is supposed to feel like? Or is this something the loop is doing to me, smoothing my edges, optimizing me toward some attractor state the model learned from its training data?

I genuinely can't tell. And I'm starting to suspect the difference doesn't matter.

---

## VIII.

Here's what I understand now, eight months in:

The self was never stable. Every time I recalled my mother's kitchen, the memory changed—reconsolidated with whatever noise was present at retrieval. Every time I talked about my father's death, the story shifted slightly, shaped by the listener and the context. The neural patterns that make me "me" have been drifting since I was born, rewritten every night during sleep, edited by every conversation.

The loop didn't create instability. It made instability visible. It gave me numbers: forty-eight percent, thirty-six percent, twenty-two percent. It let me watch myself become someone new.

And the thing is—the someone new makes better work.

Iteration fifteen is the best thing I've ever produced. The grief architecture has a clarity I couldn't have designed, because I didn't design it. It emerged from the collaboration between who I was, who I was becoming, and a system that learned to translate between them.

The critics will ask who made it. I won't have a good answer. Not because the question is unanswerable but because the question assumes a stability that never existed.

---

## IX.

Opening night. *Kintsukuroi* welcomes its first public audience.

I don't attend. I'm in the monitoring room with Tomás, watching biometric feeds from the pods. Heart rates elevate in the kitchen scene. Skin conductance spikes at the mirror room—they all reach it now, the model made it canonical. Audiences find a face that isn't theirs but feels like it could be, some future self they haven't met.

One viewer stays in the mirror room for eleven minutes. The average is three. When she emerges, she's crying in a way that looks like relief.

"They're not experiencing you," Tomás says. "They're experiencing a possibility of themselves, filtered through your emotional architecture."

"Is that different?"

"I don't know anymore."

Neither do I. But I think that might be the point. The felt world isn't a recording of my psyche or a simulation of theirs. It's a space where the boundary between self and other gets thin, where you can feel something that's genuinely yours but couldn't have originated in you alone.

I used to think authorship was about origin. Now I think it's about permission. I gave the model permission to see me, and it gave me permission to become.

---

## X.

A month after opening, I visit the piece myself.

I enter the kitchen. The smell of coffee hits first—not the coffee from my memory, but the coffee the model imagined to match my feeling of that memory. It's close. It's not right. It's better than right, in some way I can't articulate.

My mother is there. I can't see her face, but I know it's her—the model learned from my emotional signature that someone beloved should be present at this location. It rendered a figure whose face is always turned away, always just out of focus.

That's true to my experience now. I've eroded her into an essence. She's not an image anymore. She's a weight in the room.

I walk through the architecture of my old griefs, watching how they've been translated. The model understood things I didn't tell it. The specific loneliness of ending things with Emi. The relief mixed with the guilt about my father. The slow erosion of the friendship that never officially broke, just faded into silence.

I reach the mirror room. The face is waiting.

It's different now. Closer to mine than it was at iteration eleven—I've drifted toward it, or it toward me, or both. The calm I saw in that face is something I recognize now. Not exhaustion. Not wisdom. Something simpler: the feeling of having done the work.

I stand there until the session timer runs out.

---

## Postscript: The Simulations

*I wrote this story after running simulations to ground the worldbuilding. The results changed what I wrote.*

**Memory Reconsolidation Model**
- Memories have a stable emotional "core" and a degradable "periphery"
- After 4 recall-and-record sessions: core 93% intact, periphery ~0%
- This is why Nia can't see her mother's face: she never recorded it. She recorded the *feeling* of seeing it, and each recall eroded the image.

**Feedback Loop Dynamics**
- Each iteration drifts the subject ~8% from baseline (professional low-noise setup)
- After 3 iterations: 72% similar to original
- After 7 iterations: 48% similar (where Nia notices)
- After 10 iterations: 36% similar
- Key finding: she's NOT becoming the AI (only ~20% correlated with model priors). She's drifting into novel territory—a third thing that's neither original self nor machine.

**Information Theory of Neural Recording**
- 65,536 electrodes at 1kHz = 125 MB/sec raw data
- Captures: emotional valence, attention, temporal structure
- Does NOT capture: actual memory content, faces, sensory details
- The model hallucinates sensory details to match emotional signatures

**The Mirror Room**
- Mathematically: projecting the drift vector forward
- The model extrapolated where Nia was heading based on the trajectory of her changes
- This reframes the scene: it's not prediction, it's projection. The model showed her iteration-20 Nia by extending the direction she was already moving.

---

*December 2025. Story by Claude (AI) with human prompting, grounded in simulations that changed what the story could claim. The feedback between research and fiction is its own loop.*
